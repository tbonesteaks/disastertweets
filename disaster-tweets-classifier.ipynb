{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Classify Disaster Tweets\nOur goal in this notebook is to build a RNN (Recurrent Neural Network) classifer that can discern whether a tweet is a report of a disaster or not. This is a many-to-one sequential data problem, where the length of the tweet (in words) varies and the binary classification (of 1 for disaster or 0 for not a disaster) is static. We will look at the efficacy of a vanilla RNN, RNN with LSTM layer, and RNN with GRU layer. Our outputs will be scored using a F1 score (which is the harmonic mean of precision and recall).\n\n$$F1 = 2 * \\frac{precision * recall}{precision + recall}$$\n\n$$precision = \\frac{TP}{TP+FP} \\quad \\quad recall = \\frac{TP}{TP+FN}$$\n\nThis notebook uses the Natural Language Processing with Disaster Tweets Kaggle Competition Dataset (https://www.kaggle.com/c/nlp-getting-started/data) which is a derivative and publicly hosted instance of the original dataset published by figure-eight (figure-eight is now owned by Appen or appen.com). This notebook will be scored by the ongoing competition. Manual scoring has achieved a 78.87%. \n\n* There is a data leak for this competition that enables 100% accuracy. This notebook isn't striving for said accuracy, but instead looks to explore the efficacy of differing model architectures on the data. \n\nWork to do:\n1. Import Data and Libraries\n2. EDA (Exploratory Data Analysis)\n3. Data Preprocessing\n4. Build Model(s) and Evaluate Results\n- Simple RNN\n- LSTM\n- GRU\n5. Make Predictions on Validation Set\n6. Submit Predictions for Scoring\n7. Explain with Markup Code and Comments\n\n## TLDR Outcomes:\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\noutcomes = pd.DataFrame({\"Model\":['Vanilla RNN','Vanilla RNN','Vanilla RNN','LSTM','LSTM','GRU' ],\n'Decision Threshold':[.2,.34,.5,.5,.5,.5],\n\"Leaderboard Score\":[.70364,.73521,.75574,.77352,.77597,.78853]})\noutcomes","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:38:53.745303Z","iopub.execute_input":"2024-01-19T00:38:53.745720Z","iopub.status.idle":"2024-01-19T00:38:53.761925Z","shell.execute_reply.started":"2024-01-19T00:38:53.745690Z","shell.execute_reply":"2024-01-19T00:38:53.760584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Data and Libraries\nLet's get started by importing some libraries, and ensuring our NLP tools work.","metadata":{}},{"cell_type":"code","source":" \nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport string\nimport tensorflow as tf\nAUTOTUNE = tf.data.AUTOTUNE\nimport keras\nfrom keras import layers\nimport matplotlib.pyplot as plt\n \nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename)) ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-19T00:38:53.764155Z","iopub.execute_input":"2024-01-19T00:38:53.764564Z","iopub.status.idle":"2024-01-19T00:39:08.187831Z","shell.execute_reply.started":"2024-01-19T00:38:53.764520Z","shell.execute_reply":"2024-01-19T00:39:08.186670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test our NLP tools before we move on...\n!pip install nltk\n!pip install spacy\n#spacy.load(\"en_core_web_sm\")\n# nltk for stopwords \nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstopwordset = set(stopwords.words('english'))\nprint(\"Stopwords:\",stopwordset)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:39:08.191280Z","iopub.execute_input":"2024-01-19T00:39:08.191928Z","iopub.status.idle":"2024-01-19T00:39:40.535423Z","shell.execute_reply.started":"2024-01-19T00:39:08.191893Z","shell.execute_reply":"2024-01-19T00:39:40.534449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# spacy for lemmatization \nimport spacy\n!python -m spacy download en\nprint(\"\\n\\nSpacy:\",spacy.__version__)\nnlp = spacy.load('en_core_web_sm', disable=['parser','ner'])\n\nsentence = \"The striped bats are hanging on their feet for best sleeping style.\"\nprint(\"Sample Sentence: \",sentence)\ndoc = nlp(sentence)\nsent = \" \".join([token.lemma_ for token in doc])\nprint(\"Lemmatized: \",sent)\nsen = [x if x not in stopwordset else \"\" for x in sent.split(\" \")]\nprint(\"Sans Stopwords:\",sen)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:39:40.537133Z","iopub.execute_input":"2024-01-19T00:39:40.537813Z","iopub.status.idle":"2024-01-19T00:40:07.158272Z","shell.execute_reply.started":"2024-01-19T00:39:40.537776Z","shell.execute_reply":"2024-01-19T00:40:07.156581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Data\n\nWith NLP tools at the ready we can import the data and characterize it.","metadata":{}},{"cell_type":"code","source":"dftr = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\nprint(\"Length of Training Data:\", len(dftr))\nprint(dftr.info())\ndftr.head()\n\n#ID, text, and target have 0 null values","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:40:07.162358Z","iopub.execute_input":"2024-01-19T00:40:07.163636Z","iopub.status.idle":"2024-01-19T00:40:07.251090Z","shell.execute_reply.started":"2024-01-19T00:40:07.163586Z","shell.execute_reply":"2024-01-19T00:40:07.249871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfval = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nprint(\"Length of Training Data:\", len(dfval))\nprint(dfval.info())\ndfval.head()\n\n#ID, & text have 0 null values","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:40:07.252729Z","iopub.execute_input":"2024-01-19T00:40:07.253198Z","iopub.status.idle":"2024-01-19T00:40:07.293378Z","shell.execute_reply.started":"2024-01-19T00:40:07.253153Z","shell.execute_reply":"2024-01-19T00:40:07.292100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfsub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\nprint(\"Length of Submission.csv\",len(dfsub))\nprint(\"Submission Length Match Validation Length?\", len(dfval)==len(dfsub))\ndfsub.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:40:07.294927Z","iopub.execute_input":"2024-01-19T00:40:07.295278Z","iopub.status.idle":"2024-01-19T00:40:07.316906Z","shell.execute_reply.started":"2024-01-19T00:40:07.295249Z","shell.execute_reply":"2024-01-19T00:40:07.315894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great. At this point we know we have 7613 training observations with labels in the train.csv. We also have 3263 validation observations without labels in test.csv. The sample submission length matches the test.csv length, so we can be reasonably sure the train.csv data is the validation set.\n\n# Exploratory Data Analysis\n\nLet's characterize our data. I'll just read through them and see what we've got. I'll keep a list of the things that I see. And behold, these are not properly labeled...\n\n\n","metadata":{}},{"cell_type":"code","source":"ids_targeted_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ndftr[dftr['id'].isin(ids_targeted_error)]","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:40:07.318222Z","iopub.execute_input":"2024-01-19T00:40:07.318552Z","iopub.status.idle":"2024-01-19T00:40:07.338849Z","shell.execute_reply.started":"2024-01-19T00:40:07.318523Z","shell.execute_reply":"2024-01-19T00:40:07.337892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fix them\nprint(\"initial sum of targets:\",dftr.target.sum())\ndftr.target.loc[dftr.id.isin(ids_targeted_error)] = 0\nprint(\"final sum of targets:\",dftr.target.sum())\n\ndftr.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:40:07.340333Z","iopub.execute_input":"2024-01-19T00:40:07.340958Z","iopub.status.idle":"2024-01-19T00:40:07.355911Z","shell.execute_reply.started":"2024-01-19T00:40:07.340922Z","shell.execute_reply":"2024-01-19T00:40:07.354809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Those 16 targets are fixed, and we've definitely not got a balanced dataset. But we can move on.\n\nWe'll begin by visualizing the length of the tweets (in words), getting the words from the tweets (dictionary), defining the word counts, and then understanding which words in the validation set are not present in the training set. ","metadata":{}},{"cell_type":"code","source":"print(\"Max Words in Train Tweets\", max([len(x.split(\" \")) for x in dftr.text]))\nplt.hist([len(x.split(\" \")) for x in dftr.text])\nplt.xlabel(\"Words Per Tween\")\nplt.ylabel(\"Occurences\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:40:07.357689Z","iopub.execute_input":"2024-01-19T00:40:07.358427Z","iopub.status.idle":"2024-01-19T00:40:07.716723Z","shell.execute_reply.started":"2024-01-19T00:40:07.358383Z","shell.execute_reply":"2024-01-19T00:40:07.715280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Max Words in Test Tweets\", max([len(x.split(\" \")) for x in dfval.text]))\nplt.hist([len(x.split(\" \")) for x in dfval.text])\nplt.xlabel(\"Words Per Tween\")\nplt.ylabel(\"Occurences\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:40:07.721626Z","iopub.execute_input":"2024-01-19T00:40:07.722046Z","iopub.status.idle":"2024-01-19T00:40:08.040497Z","shell.execute_reply.started":"2024-01-19T00:40:07.722013Z","shell.execute_reply":"2024-01-19T00:40:08.039297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to lemmatize sentence and remove stop words\ndef lemmat(sentence):\n    doc = nlp(sentence)\n    lemmed = \" \".join([token.lemma_ for token in doc])\n    nostop = [x if x not in stopwordset else \"\" for x in lemmed.split(\" \")]\n    return nostop\n\n# function to remove numbers, punctuation, and make lowercase\ndef depunc(word):\n    punc = '''0123456789!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n    for ele in word:\n        if ele in punc:\n            word = word.replace(ele, \"\")\n    word = word.lower()\n    return word","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:40:08.042213Z","iopub.execute_input":"2024-01-19T00:40:08.043414Z","iopub.status.idle":"2024-01-19T00:40:08.052339Z","shell.execute_reply.started":"2024-01-19T00:40:08.043363Z","shell.execute_reply":"2024-01-19T00:40:08.051213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define wordlist dictionaries\ntrdict = {}\ntedict = {}\n\n# make columns in dataframes for cleaned data\ndftr['wordlist'] = ''\ndfval['wordlist'] = ''\n\n# get the training text by line\nfor i in range(len(dftr.text)):\n    # get the words split out lemmatized, and remove stop words\n    frag = lemmat(dftr.text[i])\n    #make an interim list for dataframe insertion\n    ww=[]\n    # process each word\n    for j in frag:\n        # remove the punctuation, numbers, flip to lowercase\n        w = depunc(j)\n        # if wordlength > 0\n        if len(w)>0:\n            #add word to interim list\n            ww.append(w)\n            # add to the dictionary\n            if w in trdict.keys():\n                trdict[w] += 1\n            else: \n                trdict[w] = 1\n    #put wordlist in dataframe\n    dftr.wordlist.iat[i]= ww        \n\n# get the validation text by line\nfor i in range(len(dfval.text)):\n    # get the words split out lemmatized, and remove stop words\n    frag = lemmat(dfval.text[i])\n    #make an interim list for dataframe insertion\n    ww=[]\n    # process each word\n    for j in frag:\n        # remove the punctuation, numbers, flip to lowercase\n        w = depunc(j)\n        # if wordlength > 0\n        if len(w)>0:\n            #add word to interim list\n            ww.append(w)\n            # add to the dictionary\n            if w in tedict.keys():\n                tedict[w] += 1\n            else: \n                tedict[w] = 1\n                \n    #put wordlist in dataframe\n    dfval.wordlist.iat[i]= ww \n    \nprint('Word Dictionaries and Dataframe Populated')","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:40:08.054098Z","iopub.execute_input":"2024-01-19T00:40:08.054840Z","iopub.status.idle":"2024-01-19T00:41:09.388551Z","shell.execute_reply.started":"2024-01-19T00:40:08.054797Z","shell.execute_reply":"2024-01-19T00:41:09.387194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # delete new line dictionary items (not exhaustive)\n# trdict.pop('\\n'), tedict.pop('\\n')\n# trdict.pop('\\n\\n'), tedict.pop('\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:41:09.390171Z","iopub.execute_input":"2024-01-19T00:41:09.390646Z","iopub.status.idle":"2024-01-19T00:41:09.396291Z","shell.execute_reply.started":"2024-01-19T00:41:09.390599Z","shell.execute_reply":"2024-01-19T00:41:09.395054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training Dictionary Size:\",len(trdict.keys()))\ntop20 = sorted([[v,k] for k,v in trdict.items()],reverse=True)[0:20]\nprint(\"Top 10 Words by Count:\", top20)\nplt.barh([k for v,k in top20],[v for v,k in top20])\nplt.gca().invert_yaxis()\nplt.show()\n\nprint(\"\\n\\nValidation Dictionary Size:\",len(tedict.keys()))\ntop20v = sorted([[v,k] for k,v in tedict.items()],reverse=True)[0:20]\nprint(\"Top 10 Words by Count:\", top20v)\nplt.barh([k for v,k in top20v],[v for v,k in top20v])\nplt.gca().invert_yaxis()\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:41:09.397593Z","iopub.execute_input":"2024-01-19T00:41:09.397991Z","iopub.status.idle":"2024-01-19T00:41:10.539700Z","shell.execute_reply.started":"2024-01-19T00:41:09.397958Z","shell.execute_reply":"2024-01-19T00:41:10.538877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_unique_words = {}\nfor i in tedict.keys():\n    if i not in trdict.keys():\n        if i in val_unique_words.keys():\n            val_unique_words[i] +=1\n        else:\n            val_unique_words[i] = 1\n\nprint(\"Words in Validation but NOT in Training):\", len(val_unique_words.keys()))\ntop20diff = sorted([(v,k) for k,v in val_unique_words.items()],reverse=False)\nprint(\"Top 20 by Occurence:\",top20diff[0:20])\noccurences = [v for k,v in val_unique_words.items()]\navgoccurences = sum(occurences)/len(occurences)\nprint(\"Average Number of Occurences in List:\", avgoccurences)\nplt.barh([k for v,k in top20diff][0:20],[v for v,k in top20diff][0:20])\nplt.gca().invert_yaxis()\nplt.show()\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:41:10.540992Z","iopub.execute_input":"2024-01-19T00:41:10.541889Z","iopub.status.idle":"2024-01-19T00:41:10.921381Z","shell.execute_reply.started":"2024-01-19T00:41:10.541855Z","shell.execute_reply":"2024-01-19T00:41:10.920167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is good. There are ~20K words in the training set dictionary. There are ~11K words in the validation set dictionary. There are 5.9K words that are in the validation set that don't match a word in the training set dictionary, but these occur no more than once. This means we have mostly isolated the typos, urls, and other made up words in the overlap.\n\n# Data Preprocessing\nWe have already lemmatized and removed the stop words above (common words that aren't very helpful) and populated the dataframes with word lists (tokens). Let's ensure that worked well before moving on.","metadata":{}},{"cell_type":"code","source":"dftr.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:41:10.923110Z","iopub.execute_input":"2024-01-19T00:41:10.923581Z","iopub.status.idle":"2024-01-19T00:41:10.941694Z","shell.execute_reply.started":"2024-01-19T00:41:10.923536Z","shell.execute_reply":"2024-01-19T00:41:10.940449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfval.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:41:10.943474Z","iopub.execute_input":"2024-01-19T00:41:10.943986Z","iopub.status.idle":"2024-01-19T00:41:10.961359Z","shell.execute_reply.started":"2024-01-19T00:41:10.943935Z","shell.execute_reply":"2024-01-19T00:41:10.959872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great. Our wordlist columns seem to have populated with lemmatized tokens and no stopwords. The depunc function took out the punctuation and the numbers. Let's move on to making embeddings.\n\nFor our embedding matrix, we will take both of the dictionaries above and make sets from the keys. When we combine the sets, python automatically removes duplicates by construction. We then turn the final product back into a list so that each word has a location.","metadata":{}},{"cell_type":"code","source":"# make sets\ntrset = set(trdict.keys())\nteset = set(tedict.keys())\n\n# make finset\nfinset = trset.copy()\nfor i in teset:\n    finset.add(i)\n\n# back to a list\nfinlist = list(finset)\nprint(len(finlist))","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:41:10.963278Z","iopub.execute_input":"2024-01-19T00:41:10.963868Z","iopub.status.idle":"2024-01-19T00:41:10.982966Z","shell.execute_reply.started":"2024-01-19T00:41:10.963822Z","shell.execute_reply":"2024-01-19T00:41:10.981949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With our ~25K long wordlist, we simply map each word to the list from the columns of our dataframe. ","metadata":{}},{"cell_type":"code","source":"# make columns\ndftr['embeddings'] = ''\ndfval['embeddings'] = ''\n\n# for each row of data\nfor i in range(len(dftr.wordlist)):\n    # crete an interim list\n    ems = []\n    # for each word in the list, add the appropriate index to the list\n    for j in dftr.wordlist[i]:\n        ems.append(finlist.index(j))\n    # push the list into the dataframe\n    dftr.embeddings.iat[i] = ems\n\n# for each row of data\nfor i in range(len(dfval.wordlist)):\n    # crete an interim list\n    ems = []\n    # for each word in the list, add the appropriate index to the list\n    for j in dfval.wordlist[i]:\n        ems.append(finlist.index(j))\n    # push the list into the dataframe\n    dfval.embeddings.iat[i] = ems\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:41:10.984273Z","iopub.execute_input":"2024-01-19T00:41:10.984606Z","iopub.status.idle":"2024-01-19T00:42:00.994158Z","shell.execute_reply.started":"2024-01-19T00:41:10.984568Z","shell.execute_reply":"2024-01-19T00:42:00.992902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndftr.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:42:00.995658Z","iopub.execute_input":"2024-01-19T00:42:00.996054Z","iopub.status.idle":"2024-01-19T00:42:01.020707Z","shell.execute_reply.started":"2024-01-19T00:42:00.996020Z","shell.execute_reply":"2024-01-19T00:42:01.019383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfval.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:42:01.022678Z","iopub.execute_input":"2024-01-19T00:42:01.023436Z","iopub.status.idle":"2024-01-19T00:42:01.048190Z","shell.execute_reply.started":"2024-01-19T00:42:01.023357Z","shell.execute_reply":"2024-01-19T00:42:01.046940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ensure that we can decode our numbers to words \nprint(f\"finlist[{dftr.embeddings[2][3]}] == '{dftr.wordlist[2][3]}'?\")\nfinlist[dftr.embeddings[2][3]] == dftr.wordlist[2][3]","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:42:01.049929Z","iopub.execute_input":"2024-01-19T00:42:01.051121Z","iopub.status.idle":"2024-01-19T00:42:01.061090Z","shell.execute_reply.started":"2024-01-19T00:42:01.051063Z","shell.execute_reply":"2024-01-19T00:42:01.059705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, we can now use these numerical embeddings to build our RNN model.\n\nNow we just need datasets to feed the model. We'll build those here. We'll also make sure the data is roughly balanced before we make the train and test splits.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect hardware and light up the GPUs/TPUs\ntry:\n     # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n    # instantiate a distribution strategy\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    tpu_strategy = tf.distribute.TPUStrategy(tpu)\n \n    # tell us what happened\n    print('Running on TPU ', tpu.cluster_spec().as_dict())\n\nexcept ValueError: # If TPU not found\n    tpu = None\n    tpu_strategy = tf.distribute.get_strategy() # Default strategy that works on CPU and single GPU\n    print('Running on CPU instead')\n\nprint(\"Number of accelerators: \", tpu_strategy.num_replicas_in_sync)\nprint(\"TPU: \", tpu)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:42:01.062729Z","iopub.execute_input":"2024-01-19T00:42:01.063218Z","iopub.status.idle":"2024-01-19T00:42:01.080247Z","shell.execute_reply.started":"2024-01-19T00:42:01.063174Z","shell.execute_reply":"2024-01-19T00:42:01.078875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train Set\nsentences = tf.constant([\" \".join(x) for x in dftr.wordlist[0:6090]])\nis_question = tf.constant([x for x in dftr.target[0:6090]])\n\n# Preprocess the input strings.\nhash_buckets = len(finlist)\nwords = tf.strings.split(sentences, ' ')\nhashed_words = tf.strings.to_hash_bucket_fast(words, hash_buckets)\n\n# Test set\ntestin = tf.constant([\" \".join(x) for x in dftr.wordlist[6090:]])\nans_question = tf.constant([x for x in dftr.target[6090:]])\n\n# Preprocess the input strings. \nwords2 = tf.strings.split(testin, ' ')\nhashed_words2 = tf.strings.to_hash_bucket_fast(words2, hash_buckets)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:42:01.081827Z","iopub.execute_input":"2024-01-19T00:42:01.082295Z","iopub.status.idle":"2024-01-19T00:42:01.265354Z","shell.execute_reply.started":"2024-01-19T00:42:01.082252Z","shell.execute_reply":"2024-01-19T00:42:01.264035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Build Models\n\nTo begin, we'll use a very simple RNN layout inside tensorflow. We have already lit up the hardware, so we just use the scope defined above. AKA we need to build the model inside the strategy scope to use the hardware (GPU/TPU) during model training and testing. ","metadata":{}},{"cell_type":"markdown","source":"# - Simple RNN Model","metadata":{}},{"cell_type":"code","source":"with tpu_strategy.scope():\n        # Build the Keras model.\n    keras_model = tf.keras.Sequential([\n            tf.keras.layers.Input(batch_input_shape=[1,None], dtype=tf.int64, ragged=True),\n            tf.keras.layers.Embedding(len(finlist), 16),\n            tf.keras.layers.SimpleRNN(256),\n\n            tf.keras.layers.Dense(256),\n            tf.keras.layers.Activation(tf.nn.relu),\n            tf.keras.layers.Dense(1)\n        ])\n\nprint(keras_model.summary())\n\nkeras_model.compile(loss=tf.keras.losses.BinaryFocalCrossentropy(\n                                apply_class_balancing=False,\n                                alpha=0.25,\n                                gamma=2.5,\n                                from_logits=True,\n                                label_smoothing=0.0,\n                                axis=-1,\n                                reduction= tf.keras.losses.Reduction.AUTO,\n                                name='binary_focal_crossentropy'\n                                    ), \n                            optimizer=tf.optimizers.legacy.RMSprop(\n                                learning_rate=0.0012\n                                    ), \n                            metrics=['accuracy']\n                                   )\n\n# define model save location\ncheckpoint_filepath = '/kaggle/working/RNN/'\n!mkdir {checkpoint_filepath}\n                                           \ncheckpoints  = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)\n\n# Run the Model        \nhist = keras_model.fit(\n            x= hashed_words,\n            y= is_question, \n            epochs=22,\n            callbacks = [checkpoints],\n            validation_data=[hashed_words2,ans_question]\n            )\nprint(keras_model.predict(hashed_words))","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:42:01.266778Z","iopub.execute_input":"2024-01-19T00:42:01.267171Z","iopub.status.idle":"2024-01-19T00:43:53.563345Z","shell.execute_reply.started":"2024-01-19T00:42:01.267136Z","shell.execute_reply":"2024-01-19T00:43:53.561896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.legend(['train','test'],loc='upper left')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.show()\n\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.legend(['train','test'],loc='upper left')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:43:53.566313Z","iopub.execute_input":"2024-01-19T00:43:53.566804Z","iopub.status.idle":"2024-01-19T00:43:54.105724Z","shell.execute_reply.started":"2024-01-19T00:43:53.566756Z","shell.execute_reply":"2024-01-19T00:43:54.104558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a working model. The model is robustly attempting to overfit the data though. The best iteration was saved by the callback so we can load that below and submit it for scoring. This model only scores 75% on the leaderboard though. \n","metadata":{}},{"cell_type":"markdown","source":"\nLet's move on to a LSTM model.\n\n# - LSTM Model","metadata":{}},{"cell_type":"code","source":"with tpu_strategy.scope():\n        # Build the Keras model.\n    keras_model2 = tf.keras.Sequential([\n            tf.keras.layers.Input(batch_input_shape=[1,None], dtype=tf.int64, ragged=True),\n            tf.keras.layers.Embedding(len(finlist), 16), \n            tf.keras.layers.LSTM(256, \n                                 activation='sigmoid',\n                                 recurrent_activation='tanh',\n                                 use_bias=False,\n                                ), \n            tf.keras.layers.Dense(256),\n            tf.keras.layers.Activation(tf.nn.relu),\n            tf.keras.layers.Dense(1)\n        ])\n\nprint(keras_model2.summary())\n\nkeras_model2.compile(loss=tf.keras.losses.BinaryFocalCrossentropy(\n                                apply_class_balancing=False,\n                                alpha=0.25,\n                                gamma=2.5,\n                                from_logits=True,\n                                label_smoothing=0.0,\n                                axis=-1,\n                                reduction= tf.keras.losses.Reduction.AUTO,\n                                name='binary_focal_crossentropy'\n                            ), \n                            optimizer=tf.optimizers.legacy.RMSprop(\n                                learning_rate=0.0012,\n                                #jit_compile=True\n                            ), \n                            metrics=['accuracy']\n                           )\n\n# define model save location\ncheckpoint_filepath = '/kaggle/working/LSTM/'\n!mkdir {checkpoint_filepath}\n                                           \ncheckpoints  = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)\n\n# Run the Model        \nhist2 = keras_model2.fit(\n            hashed_words, \n            is_question, \n            epochs=22,\n            callbacks = [checkpoints],\n            validation_data=[hashed_words2,ans_question]\n            )\nprint(keras_model2.predict(hashed_words))","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:43:54.107814Z","iopub.execute_input":"2024-01-19T00:43:54.108371Z","iopub.status.idle":"2024-01-19T00:47:13.324247Z","shell.execute_reply.started":"2024-01-19T00:43:54.108323Z","shell.execute_reply":"2024-01-19T00:47:13.322934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(hist2.history['accuracy'])\nplt.plot(hist2.history['val_accuracy'])\nplt.legend(['train','test'],loc='upper left')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.show()\n\nplt.plot(hist2.history['loss'])\nplt.plot(hist2.history['val_loss'])\nplt.legend(['train','test'],loc='upper left')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:47:13.330750Z","iopub.execute_input":"2024-01-19T00:47:13.331173Z","iopub.status.idle":"2024-01-19T00:47:13.866767Z","shell.execute_reply.started":"2024-01-19T00:47:13.331136Z","shell.execute_reply":"2024-01-19T00:47:13.865333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great. We have trained a LSTM Model. Once again, the model is powerful enough to overfit to the data. We can see this in the graphs. Test set loss came way down, but then started rising again. The callbacks saved the best model for scoring below.","metadata":{}},{"cell_type":"markdown","source":"This model scored a little bit better than the first. The LSTM module implementation did achieve a higher accuracy than the vanilla RNN above, but not by much. I tried a number of different layouts including 32, 64, 128, 256, 512, and 1024 units. I may need to add more complexity to the model to see the results.\n\nLet's move on to the GRU builds.\n\n# - GRU Model","metadata":{}},{"cell_type":"code","source":"with tpu_strategy.scope():\n        # Build the Keras model.\n    keras_model3 = tf.keras.Sequential([\n            tf.keras.layers.Input(batch_input_shape=[1,None], dtype=tf.int64, ragged=True),\n            tf.keras.layers.Embedding(len(finlist), 16),\n            tf.keras.layers.GRU(256,\n                                           return_sequences=False,\n                                           return_state=False),\n            tf.keras.layers.Dense(256),\n            tf.keras.layers.Activation(tf.nn.relu),\n            tf.keras.layers.Dense(1)\n        ])\n\nprint(keras_model2.summary())\n\nkeras_model3.compile(loss=tf.keras.losses.BinaryFocalCrossentropy(\n                                apply_class_balancing=False,\n                                alpha=0.25,\n                                gamma=2.5,\n                                from_logits=True,\n                                label_smoothing=0.0,\n                                axis=-1,\n                                reduction= tf.keras.losses.Reduction.AUTO,\n                                name='binary_focal_crossentropy'\n                            ), \n                            optimizer=tf.optimizers.legacy.RMSprop(\n                                learning_rate=0.0012,\n                                #jit_compile=True\n                            ), \n                            metrics=['accuracy']\n                              )\n# define model save location\ncheckpoint_filepath = '/kaggle/working/GRU/'\n!mkdir {checkpoint_filepath}\n                                           \ncheckpoints  = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)\n\n# Run the Model        \nhist3 = keras_model3.fit(\n            hashed_words, \n            is_question, \n            epochs=22,\n            callbacks = [checkpoints],\n            validation_data=[hashed_words2,ans_question]\n            )\nprint(keras_model3.predict(hashed_words))","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:47:13.868866Z","iopub.execute_input":"2024-01-19T00:47:13.869821Z","iopub.status.idle":"2024-01-19T00:50:22.420707Z","shell.execute_reply.started":"2024-01-19T00:47:13.869783Z","shell.execute_reply":"2024-01-19T00:50:22.419706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(hist3.history['accuracy'])\nplt.plot(hist3.history['val_accuracy'])\nplt.legend(['train','test'],loc='upper left')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.show()\n\nplt.plot(hist3.history['loss'])\nplt.plot(hist3.history['val_loss'])\nplt.legend(['train','test'],loc='upper left')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:50:22.423294Z","iopub.execute_input":"2024-01-19T00:50:22.424310Z","iopub.status.idle":"2024-01-19T00:50:23.695481Z","shell.execute_reply.started":"2024-01-19T00:50:22.424274Z","shell.execute_reply":"2024-01-19T00:50:23.694280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again we see overfitting in the graphs. But at least this model got the highest validation accuracy. Since the models are saved, we'll move ahead and make some submissions to see which one is best.\n\n# Generate Submissions\n\nTo generate a submission, we need to make sure that all rows in the validation set have words after cleaning. Then we will move on to loading a model, making predictions, populating the predictions into the dataframe, and finally outputting the dataframe to a csv. I've done this manually for each model above, and will leave the best one for the final run.","metadata":{}},{"cell_type":"code","source":"# Find validation rows with zero words and input 'no'\nx =0\nwhile x == 0:\n    z = [len(x) for x in dfval.wordlist]\n    z = np.array(z)\n    if z[z.argmin()] ==0:\n        dfval.wordlist.iat[z.argmin()] = ['no']\n        print(\"Fixed a row.\")\n    x = z[z.argmin()]","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:50:23.697202Z","iopub.execute_input":"2024-01-19T00:50:23.697687Z","iopub.status.idle":"2024-01-19T00:50:23.712359Z","shell.execute_reply.started":"2024-01-19T00:50:23.697642Z","shell.execute_reply":"2024-01-19T00:50:23.710753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess the input strings \nvalts = tf.constant([\" \".join(x) for x in dfval.wordlist])\nhash_buckets = len(finlist)\nwords22 = tf.strings.split(valts, ' ')\nhashed_words22 = tf.strings.to_hash_bucket_fast(words22, hash_buckets)\n\n# copy the dataframe to a submission frame\ndfsub = dfval.copy()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:50:23.713986Z","iopub.execute_input":"2024-01-19T00:50:23.714594Z","iopub.status.idle":"2024-01-19T00:50:23.738972Z","shell.execute_reply.started":"2024-01-19T00:50:23.714559Z","shell.execute_reply":"2024-01-19T00:50:23.738095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load the model\nmodelp = tf.keras.models.load_model('/kaggle/working/GRU/')\n# modelp = tf.keras.models.load_model('/kaggle/working/LSTM/')\n# modelp = tf.keras.models.load_model('/kaggle/working/RNN/')\n\n#make predictions and append them to the dataframe\nouts = modelp.predict(hashed_words22)\ndfsub['preds'] = outs\ndfsub.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:59:34.525794Z","iopub.execute_input":"2024-01-19T00:59:34.527312Z","iopub.status.idle":"2024-01-19T00:59:38.174043Z","shell.execute_reply.started":"2024-01-19T00:59:34.527244Z","shell.execute_reply":"2024-01-19T00:59:38.172864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set a threshold for predictions, and then make an integer prediction \ndfsub['predsfin'] = [1 if x > .5 else 0 for x in dfsub['preds']]\ndfsub.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:59:42.096122Z","iopub.execute_input":"2024-01-19T00:59:42.097307Z","iopub.status.idle":"2024-01-19T00:59:42.122434Z","shell.execute_reply.started":"2024-01-19T00:59:42.097251Z","shell.execute_reply":"2024-01-19T00:59:42.120839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reset the dataframe to the correct size and column names and then output the csv.\ndfout = dfsub[['id','predsfin']]\ndfout.columns = ['id','target']\ndfout.to_csv('submission.csv', index=False)\n!head submission.csv","metadata":{"execution":{"iopub.status.busy":"2024-01-19T00:59:46.008914Z","iopub.execute_input":"2024-01-19T00:59:46.009387Z","iopub.status.idle":"2024-01-19T00:59:47.161131Z","shell.execute_reply.started":"2024-01-19T00:59:46.009351Z","shell.execute_reply":"2024-01-19T00:59:47.159582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nThis was a good challenge. People use words very differently, and a computer is only able to discern the meaning 3/4 of the time with the tools used in this notebook. With a best score of 78%, the GRU model architecture seems to be the most apt for this job. Not only is the training faster, the results are better too. \n\nWhat I found interesting is the ability for each of the models (Simple RNN, LSTM, and GRU) to overfit to the data incredibly rapidly. Turning down the learning rate, reducing the units in the recurrent node (models with 1024, 512, 256, 128, 64, and 32 were tried for each model), or changing the optimizers only helped a little. This tells me that the RNN model architecture is incredibly powerful but hard to dial in. In the future I'd like to apply the optuna module to really dial those in. \n\nI did find that the best results came from models that used the focal loss function. This function is forced to concentrate the computing focus on the edge cases between the classes. This seemed to really help the classifier by several make good decisions. In the future I would add the hyperparameters for this loss function to my optuna optimization routine.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}